# -*- coding: utf-8 -*-
"""Linear Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iYgjrMBg01YC-m6ZfG7a_ozyCTqWOqQ9
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
df = pd.read_csv("dataset_200x4_regression.csv", header=None)
df.columns = ['x1','x2','x3','y']
df.head()

"""# **Utilities**"""

# Linear Regression

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Utilities
def visualize_cost(cost_history):
    plt.xlabel('Iteration')
    plt.ylabel('Cost')
    plt.grid()
    plt.plot(list(range(len(cost_history))), cost_history, '-r')
    plt.show()

def do_predictions(X, t, optimal_weights):
    examples, _ = X.shape
    pred = np.dot(X, optimal_weights)
    error = pred - t
    cost = np.sum(error ** 2) / (2 * examples)
    print(f'Cost function is {cost}')

def column_investigation(data):
    sns.pairplot(data, x_vars=['Feat1', 'Feat2', 'Feat3'], y_vars='Target',
                 height=4, aspect=1, kind='scatter')
    plt.show()

# Core functions
def add_intercept(X):
    return np.hstack([np.ones((X.shape[0],1)), X])

def mse_cost(X, y, w):
    N = X.shape[0]
    preds = X.dot(w)
    return np.mean((preds - y)**2) / 2.0

def gradient_descent_linear_regression(X, y, step_size=0.01, precision=1e-4,
                                       max_iter=1000, init_w=None, verbose=False):
    N, D = X.shape
    if init_w is None:
        w = np.random.rand(D)
    else:
        w = init_w.astype(float).copy()
    costs = []
    for it in range(max_iter):
        preds = X.dot(w)
        error = preds - y
        grad = X.T.dot(error) / N
        w_new = w - step_size * grad
        cost = mse_cost(X, y, w_new)
        costs.append(cost)
        if np.linalg.norm(w_new - w, ord=2) < precision:
            w = w_new
            return w, costs, it+1
        w = w_new
    return w, costs, max_iter

def normal_equations_solution(X, y):
    XtX = X.T.dot(X)
    Xty = X.T.dot(y)
    try:
        w = np.linalg.solve(XtX, Xty)
    except np.linalg.LinAlgError:
        w = np.linalg.pinv(XtX).dot(Xty)
    return w

def numeric_gradient_check(X, y, w, eps=1e-4):
    D = w.size
    numeric_grad = np.zeros_like(w)
    for i in range(D):
        w_plus = w.copy(); w_minus = w.copy()
        w_plus[i] += eps
        w_minus[i] -= eps
        cost_plus = mse_cost(X, y, w_plus)
        cost_minus = mse_cost(X, y, w_minus)
        numeric_grad[i] = (cost_plus - cost_minus) / (2*eps)
    preds = X.dot(w)
    analytic_grad = X.T.dot(preds - y) / X.shape[0]
    return numeric_grad, analytic_grad

# Load CSV
csv_files = [f for f in os.listdir('.') if f.lower().endswith('.csv')]
if len(csv_files) == 0:
    raise FileNotFoundError("No CSV file found. Please upload the dataset first.")
data_filename = csv_files[0]
print("Using CSV file:", data_filename)

# Read dataset (assuming first row is header, first 3 columns are features, 4th column is target)
df = pd.read_csv(data_filename, header=0)
df = df.iloc[:, :4].copy()
df.columns = ['Feat1','Feat2','Feat3','Target']
print("Data sample:")
display(df.head())

X_raw = df[['Feat1','Feat2','Feat3']].values
y_raw = df['Target'].values

column_investigation(df)

# P1: Toy example
xtoy = np.array([0,0.2,0.4,0.8,1.0])
ttoy = 5 + xtoy
Xtoy = add_intercept(xtoy.reshape(-1,1))
w_toy, costs_toy, it_toy = gradient_descent_linear_regression(
    Xtoy, ttoy, step_size=0.1, precision=1e-5, max_iter=1000)
print("\nP1 Toy example weights:", w_toy, "iters:", it_toy)
visualize_cost(costs_toy)

# Scale data
mmX = MinMaxScaler(); mmy = MinMaxScaler()
X_scaled = mmX.fit_transform(X_raw)
y_scaled = mmy.fit_transform(y_raw.reshape(-1,1)).ravel()
Xs_int = add_intercept(X_scaled)

# Verification (start [1,1,1,1], 3 iterations)
init_ver = np.array([1.,1.,1.,1.])
w_ver, costs_ver, _ = gradient_descent_linear_regression(
    Xs_int, y_scaled, step_size=0.1, precision=1e-6, max_iter=3, init_w=init_ver)
print("\nVerification after 3 iterations:", w_ver)
print("Costs:", costs_ver)

# P2: Hyperparameter search
step_sizes = [1e-1,1e-2,1e-3,1e-4,1e-5,1e-7]
precisions = [1e-2,1e-3,1e-4,1e-5]
best = {"error": np.inf}
np.random.seed(0)
for step in step_sizes:
    for prec in precisions:
        for run in range(3):
            init_w = np.random.rand(Xs_int.shape[1])
            w, costs, iters = gradient_descent_linear_regression(
                Xs_int, y_scaled, step_size=step, precision=prec,
                max_iter=10000, init_w=init_w)
            final_cost = costs[-1]
            if final_cost < best["error"]:
                best = {"step": step, "precision": prec, "run": run,
                        "error": final_cost, "iters": iters,
                        "w": w.copy(), "costs": costs.copy()}

print("\nP2 best hyperparams:", best)
visualize_cost(best["costs"])

# P3: do_predictions on best model
do_predictions(Xs_int, y_scaled, best["w"])

# P4: scaling vs not scaling vs standardization
X_ns = add_intercept(X_raw)
w_ns, costs_ns, it_ns = gradient_descent_linear_regression(
    X_ns, y_raw, step_size=0.1, precision=1e-6, max_iter=5000,
    init_w=np.random.rand(X_ns.shape[1]))
print("\nP4 No-scaling final cost:", costs_ns[-1])

std_scaler = StandardScaler()
X_std = std_scaler.fit_transform(X_raw)
X_std_int = add_intercept(X_std)
w_std, costs_std, it_std = gradient_descent_linear_regression(
    X_std_int, y_scaled, step_size=0.1, precision=1e-6, max_iter=5000,
    init_w=np.random.rand(X_std_int.shape[1]))
print("P4 StandardScaler final cost:", costs_std[-1])

# P5: correlation + best single feature
corrs = [np.corrcoef(df[c], df['Target'])[0,1] for c in ['Feat1','Feat2','Feat3']]
print("\nFeature correlations:", corrs)
best_idx = int(np.argmax(np.abs(corrs)))
best_feature = ['Feat1','Feat2','Feat3'][best_idx]
print("P5 Best feature:", best_feature)

X_bf = add_intercept(df[[best_feature]].values)
w_bf, costs_bf, it_bf = gradient_descent_linear_regression(
    X_bf, y_raw, step_size=0.1, precision=1e-6, max_iter=10000,
    init_w=np.random.rand(2))
print("Single-feature weights:", w_bf, "final cost:", costs_bf[-1])

# P6: normal equations
w_ne_scaled = normal_equations_solution(Xs_int, y_scaled)
w_ne_raw = normal_equations_solution(add_intercept(X_raw), y_raw)
print("\nP6 Normal eq scaled:", w_ne_scaled)
print("P6 Normal eq raw:", w_ne_raw)

# P7: gradient check
subset = np.random.choice(Xs_int.shape[0], size=10, replace=False)
numg, ang = numeric_gradient_check(Xs_int[subset], y_scaled[subset],
                                   np.random.rand(Xs_int.shape[1]), eps=1e-4)
print("\nP7 Gradient check (first 5 params):")
for i in range(min(5, len(numg))):
    print(f"param {i}: numeric={numg[i]:.4e}, analytic={ang[i]:.4e}, diff={abs(numg[i]-ang[i]):.2e}")

# P8 & P9: Derivation + Complexity
print("\nP8 Derivation:")
print("- For y = a + Bx, S = Σ(y - a - Bx)^2")
print("- dS/da = 0 → a = ȳ - Bx̄")
print("- dS/dB = 0 → B = Σ(x - x̄)(y - ȳ) / Σ(x - x̄)^2")

print("\nP9 Complexity:")
print("- Gradient Descent: O(k·n·d)")
print("- Normal Equation: O(n·d² + d³)")